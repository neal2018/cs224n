# Assignment 3 written part

(a) (i)

It's because $\boldsymbol{m}$ can be viewed as the weighted sum of the current and previous gradients. It can help to update the model faster when the gradients is sparse. Moreover, this low variance can be helpful to reducing the error of gradients introduced by mini-batch when the total gradients is small. 

Therefore, it would be helpful to learning.

(ii)

The parameters with lower gradients will get larger updates, which can help the model to go to the optimal solution more faster.

(b) (i)

To reduce the information loss.

(ii)

To reduce the overfitting.



